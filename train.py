"""
Robust SVM Network Attack Detection System - 20 Iterations Version
Optimized for Google Colab with T4 GPU Support - Fast Training

Features:
- 20 training iterations with data shuffling (optimized for speed)
- 5 test runs per training iteration
- Comprehensive performance tracking and analysis
- Statistical analysis of model stability
- GPU/CPU automatic detection and optimization
- Testing runs AFTER training is fully completed
- Total time: ~1 hour

Author: AI Assistant
Date: 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
from sklearn.utils import shuffle
import pickle
import warnings
import time
import os
import gc
from datetime import datetime
from tqdm import tqdm
import json

# GPU Libraries (Optional)
try:
    from cuml.svm import SVC as cuSVC
    from cuml.preprocessing import StandardScaler as cuStandardScaler
    import cudf
    import cupy as cp
    GPU_AVAILABLE = True
    print("‚úÖ GPU libraries (cuML) available - GPU acceleration enabled")
except ImportError:
    GPU_AVAILABLE = False
    print("‚ö†Ô∏è  GPU libraries not available - using CPU-based scikit-learn")

warnings.filterwarnings('ignore')

class FastRobustNetworkAttackDetector:
    """
    Fast Robust Network Attack Detection System using SVM with 20 Training Iterations
    
    Features:
    - 20 training iterations with data shuffling (fast training)
    - 5 test runs per training iteration (total: 100 evaluations)
    - Comprehensive performance tracking
    - Statistical analysis and visualization
    - Model persistence
    - Testing runs AFTER all training is completed
    - Optimized for speed while maintaining reliability
    """
    
    def __init__(self, use_gpu=True, base_random_state=42):
        """
        Initialize the Fast Robust Network Attack Detector
        
        Args:
            use_gpu (bool): Whether to use GPU acceleration if available
            base_random_state (int): Base random state for reproducibility
        """
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.base_random_state = base_random_state
        self.scaler = None
        self.label_encoder = None
        self.best_model = None
        self.feature_names = None
        self.class_names = None
        self.trained_models = []  # Store all trained models for testing
        self.training_results = []
        self.testing_results = []
        
        print(f"üöÄ Fast Robust Detector (20 iterations) initialized with {'GPU' if self.use_gpu else 'CPU'} acceleration")
    
    def load_and_preprocess_data(self, train_path, test_path):
        """
        Load and preprocess training and test data
        
        Args:
            train_path (str): Path to training CSV file
            test_path (str): Path to test CSV file
            
        Returns:
            tuple: (train_df_processed, test_df_processed, y_test_actual)
        """
        print("\n" + "="*60)
        print("üìä DATA LOADING AND PREPROCESSING")
        print("="*60)
        
        try:
            # Load datasets
            print("üìÅ Loading training data...")
            train_df = pd.read_csv(train_path)
            print("üìÅ Loading test data...")
            test_df = pd.read_csv(test_path)
            
            print(f"‚úÖ Training data shape: {train_df.shape}")
            print(f"‚úÖ Test data shape: {test_df.shape}")
            
            # Identify target column
            target_col = None
            if 'Attack' in train_df.columns:
                target_col = 'Attack'
            elif 'Label' in train_df.columns:
                target_col = 'Label'
            else:
                raise ValueError("‚ùå No target column found in training data (expected 'Attack' or 'Label')")
            
            print(f"üéØ Target column: {target_col}")
            
            # Extract actual test labels BEFORE preprocessing
            if target_col in test_df.columns:
                y_test_actual = test_df[target_col].copy()
                print(f"‚úÖ Test labels extracted: {len(y_test_actual)} samples")
            else:
                raise ValueError(f"‚ùå Target column '{target_col}' not found in test data")
            
            # Remove target column from test data
            test_df_features = test_df.drop(columns=[target_col])
            
            # Combine ONLY the feature columns for consistent preprocessing
            train_features = train_df.drop(columns=[target_col])
            combined_features = pd.concat([train_features, test_df_features], ignore_index=True)
            
            # Data cleaning
            print("üßπ Cleaning data...")
            print(f"   - Missing values: {combined_features.isnull().sum().sum()}")
            combined_features = combined_features.fillna(0)
            
            print(f"   - Infinite values: {np.isinf(combined_features.select_dtypes(include=[np.number])).sum().sum()}")
            combined_features = combined_features.replace([np.inf, -np.inf], 0)
            
            # Remove non-useful columns
            columns_to_drop = ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp']
            existing_cols_to_drop = [col for col in columns_to_drop if col in combined_features.columns]
            if existing_cols_to_drop:
                combined_features = combined_features.drop(columns=existing_cols_to_drop)
                print(f"   - Dropped columns: {existing_cols_to_drop}")
            
            # Store original class names
            y_train = train_df[target_col]
            self.class_names = sorted(y_train.unique())
            print(f"üè∑Ô∏è  Classes found: {self.class_names}")
            
            # Show class distribution
            print(f"üìä Training class distribution:")
            for class_name in self.class_names:
                count = (y_train == class_name).sum()
                percentage = (count / len(y_train)) * 100
                print(f"   - {class_name}: {count:,} ({percentage:.1f}%)")
            
            # Encode categorical features
            categorical_columns = combined_features.select_dtypes(include=['object']).columns
            if len(categorical_columns) > 0:
                print(f"üî§ Encoding {len(categorical_columns)} categorical columns...")
                for col in tqdm(categorical_columns, desc="Encoding"):
                    le = LabelEncoder()
                    combined_features[col] = le.fit_transform(combined_features[col].astype(str))
            
            # Encode target labels
            print("üéØ Encoding target labels...")
            self.label_encoder = LabelEncoder()
            y_train_encoded = self.label_encoder.fit_transform(y_train)
            y_test_encoded = self.label_encoder.transform(y_test_actual)
            
            # Split features back to train/test
            train_size = len(train_features)
            X_train_processed = combined_features.iloc[:train_size]
            X_test_processed = combined_features.iloc[train_size:]
            
            # Create processed training dataframe with encoded target
            train_df_processed = X_train_processed.copy()
            train_df_processed[target_col] = y_train_encoded
            
            # Store feature names
            self.feature_names = X_train_processed.columns.tolist()
            
            print(f"‚úÖ Preprocessing completed:")
            print(f"   - Features: {len(self.feature_names)}")
            print(f"   - Training samples: {len(X_train_processed):,}")
            print(f"   - Test samples: {len(X_test_processed):,}")
            print(f"   - Classes: {len(self.class_names)}")
            
            # Clean up memory
            del combined_features, train_df, test_df, train_features, test_df_features
            gc.collect()
            
            return train_df_processed, X_test_processed, y_test_encoded
            
        except Exception as e:
            print(f"‚ùå Error in data preprocessing: {str(e)}")
            raise
    
    def train_all_iterations(self, train_df, n_iterations=20):
        """
        Train models for all iterations FIRST, then test later
        
        Args:
            train_df: Training dataframe with target column
            n_iterations: Number of training iterations
        """
        print("\n" + "="*80)
        print(f"ü§ñ FAST TRAINING PHASE: {n_iterations} ITERATIONS")
        print("="*80)
        
        self.training_results = []
        self.trained_models = []
        
        # Progress tracking for training only
        pbar = tqdm(total=n_iterations, desc="Fast Training Progress")
        
        start_total_time = time.time()
        
        for iteration in range(n_iterations):
            # Generate random state for this iteration
            random_state = self.base_random_state + iteration
            
            try:
                # Shuffle training data with specific random state
                train_df_shuffled = shuffle(train_df, random_state=random_state)
                
                # Separate features and target
                target_col = 'Attack' if 'Attack' in train_df_shuffled.columns else 'Label'
                X_train = train_df_shuffled.drop(columns=[target_col])
                y_train = train_df_shuffled[target_col]
                
                # Scale features and train model
                start_time = time.time()
                
                if self.use_gpu:
                    scaler = cuStandardScaler()
                    X_train_gpu = cudf.DataFrame(X_train)
                    X_train_scaled = scaler.fit_transform(X_train_gpu)
                    y_train_gpu = cudf.Series(y_train)
                    
                    # Train model
                    model = cuSVC(
                        kernel='rbf',
                        C=1.0,
                        gamma='scale',
                        probability=True,
                        cache_size=2000,
                        max_iter=1000,
                        random_state=random_state
                    )
                    model.fit(X_train_scaled, y_train_gpu)
                    
                    # Calculate training accuracy
                    train_pred = model.predict(X_train_scaled)
                    train_pred_cpu = self._convert_gpu_to_cpu(train_pred)
                    y_train_cpu = self._convert_gpu_to_cpu(y_train_gpu)
                    
                else:
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    
                    model = SVC(
                        kernel='rbf',
                        C=1.0,
                        gamma='scale',
                        probability=True,
                        cache_size=1000,
                        max_iter=1000,
                        random_state=random_state
                    )
                    model.fit(X_train_scaled, y_train)
                    
                    # Calculate training accuracy
                    train_pred = model.predict(X_train_scaled)
                    train_pred_cpu = train_pred
                    y_train_cpu = y_train
                
                training_time = time.time() - start_time
                train_accuracy = accuracy_score(y_train_cpu, train_pred_cpu)
                
                # Store training results and model
                training_result = {
                    'iteration': iteration + 1,
                    'random_state': random_state,
                    'training_time': training_time,
                    'train_accuracy': train_accuracy
                }
                self.training_results.append(training_result)
                
                # Store model and scaler for later testing
                model_info = {
                    'iteration': iteration + 1,
                    'model': model,
                    'scaler': scaler,
                    'random_state': random_state,
                    'training_time': training_time,
                    'train_accuracy': train_accuracy
                }
                self.trained_models.append(model_info)
                
                # Progress update
                elapsed_total = time.time() - start_total_time
                avg_time_per_iter = elapsed_total / (iteration + 1)
                eta_minutes = (avg_time_per_iter * (n_iterations - iteration - 1)) / 60
                
                pbar.set_description(f"Training Progress (ETA: {eta_minutes:.1f}min)")
                pbar.update(1)
                
            except Exception as e:
                print(f"   ‚ùå Error in iteration {iteration + 1}: {str(e)}")
                pbar.update(1)
                continue
            
            # Memory cleanup every 5 iterations (more frequent for smaller batches)
            if (iteration + 1) % 5 == 0:
                gc.collect()
        
        pbar.close()
        
        total_training_time = time.time() - start_total_time
        
        print(f"\n‚úÖ ALL TRAINING COMPLETED!")
        print(f"   üìä Successfully trained models: {len(self.trained_models)}")
        print(f"   ‚è±Ô∏è  Total training time: {total_training_time:.2f} seconds ({total_training_time/60:.1f} minutes)")
        print(f"   üìà Average training time per iteration: {total_training_time/len(self.trained_models):.2f} seconds")
        print(f"   üìà Average training accuracy: {np.mean([r['train_accuracy'] for r in self.training_results]):.4f}")
    
    def test_all_models(self, X_test, y_test_actual, n_test_runs=5):
        """
        Test all trained models AFTER training is completed
        
        Args:
            X_test: Test features
            y_test_actual: Actual test labels
            n_test_runs: Number of test runs per model
        """
        print("\n" + "="*80)
        print(f"üß™ TESTING PHASE: {len(self.trained_models)} MODELS √ó {n_test_runs} RUNS")
        print("="*80)
        
        if not self.trained_models:
            print("‚ùå No trained models available for testing!")
            return
        
        self.testing_results = []
        best_avg_accuracy = 0
        
        # Progress tracking for testing
        total_tests = len(self.trained_models) * n_test_runs
        pbar = tqdm(total=total_tests, desc="Testing Progress")
        
        start_test_time = time.time()
        
        for model_idx, model_info in enumerate(self.trained_models):
            iteration = model_info['iteration']
            model = model_info['model']
            scaler = model_info['scaler']
            
            # Test this model multiple times
            iteration_test_results = []
            
            for test_run in range(1, n_test_runs + 1):
                try:
                    test_result = self._test_single_model(
                        model, scaler, X_test, y_test_actual, iteration, test_run
                    )
                    
                    if test_result is not None:
                        iteration_test_results.append(test_result)
                        self.testing_results.append(test_result)
                    
                except Exception as e:
                    print(f"   ‚ùå Error in testing iteration {iteration}, run {test_run}: {str(e)}")
                
                # Update progress with ETA
                completed_tests = model_idx * n_test_runs + test_run
                elapsed_test = time.time() - start_test_time
                if completed_tests > 0:
                    avg_test_time = elapsed_test / completed_tests
                    eta_test_minutes = (avg_test_time * (total_tests - completed_tests)) / 60
                    pbar.set_description(f"Testing Progress (ETA: {eta_test_minutes:.1f}min)")
                
                pbar.update(1)
            
            # Check if this is the best model so far
            if iteration_test_results:
                avg_accuracy = np.mean([r['accuracy'] for r in iteration_test_results])
                
                if avg_accuracy > best_avg_accuracy:
                    best_avg_accuracy = avg_accuracy
                    self.best_model = model
                    self.scaler = scaler
        
        pbar.close()
        
        total_test_time = time.time() - start_test_time
        
        print(f"\n‚úÖ ALL TESTING COMPLETED!")
        print(f"   üìä Total test runs: {len(self.testing_results)}")
        print(f"   ‚è±Ô∏è  Total testing time: {total_test_time:.2f} seconds ({total_test_time/60:.1f} minutes)")
        print(f"   üèÜ Best average accuracy: {best_avg_accuracy:.4f}")
    
    def _test_single_model(self, model, scaler, X_test, y_test_actual, iteration, test_run):
        """
        Test a single model for one test run
        
        Args:
            model: Trained model
            scaler: Fitted scaler
            X_test: Test features
            y_test_actual: Actual test labels
            iteration: Training iteration number
            test_run: Current test run (1-5)
            
        Returns:
            dict: Test results
        """
        try:
            # Scale test features
            if self.use_gpu:
                X_test_gpu = cudf.DataFrame(X_test)
                X_test_scaled = scaler.transform(X_test_gpu)
                
                # Make predictions
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)
                
                # Convert to CPU
                y_pred_cpu = self._convert_gpu_to_cpu(y_pred)
                y_pred_proba_cpu = self._convert_gpu_to_cpu(y_pred_proba)
            else:
                X_test_scaled = scaler.transform(X_test)
                y_pred_cpu = model.predict(X_test_scaled)
                y_pred_proba_cpu = model.predict_proba(X_test_scaled)
            
            # Ensure proper format
            if hasattr(y_pred_cpu, 'values'):
                y_pred_cpu = y_pred_cpu.values
            if hasattr(y_pred_proba_cpu, 'values'):
                y_pred_proba_cpu = y_pred_proba_cpu.values
            
            # Calculate metrics
            accuracy = accuracy_score(y_test_actual, y_pred_cpu)
            precision, recall, f1, support = precision_recall_fscore_support(y_test_actual, y_pred_cpu, average='weighted')
            
            # Per-class accuracy
            per_class_accuracy = []
            for i in range(len(self.class_names)):
                mask = (y_test_actual == i)
                if np.sum(mask) > 0:
                    class_acc = np.mean(y_pred_cpu[mask] == y_test_actual[mask])
                    per_class_accuracy.append(class_acc)
                else:
                    per_class_accuracy.append(0.0)
            
            # Prediction confidence
            max_confidence = np.max(y_pred_proba_cpu, axis=1)
            avg_confidence = np.mean(max_confidence)
            
            return {
                'iteration': iteration,
                'test_run': test_run,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'per_class_accuracy': per_class_accuracy,
                'avg_confidence': avg_confidence,
                'predictions': y_pred_cpu,
                'probabilities': y_pred_proba_cpu
            }
            
        except Exception as e:
            return None
    
    def analyze_results(self):
        """
        Analyze and visualize results from all iterations
        """
        print("\n" + "="*60)
        print("üìä ANALYZING FAST ROBUST TRAINING RESULTS (20 ITERATIONS)")
        print("="*60)
        
        if not self.training_results or not self.testing_results:
            print("‚ùå No results to analyze!")
            return None, None, None
        
        # Convert results to DataFrames
        train_df = pd.DataFrame(self.training_results)
        test_df = pd.DataFrame(self.testing_results)
        
        # Calculate statistics
        print("üìà Training Statistics:")
        print(f"   ‚Ä¢ Total iterations: {len(train_df)}")
        print(f"   ‚Ä¢ Average training time: {train_df['training_time'].mean():.2f} ¬± {train_df['training_time'].std():.2f} seconds")
        print(f"   ‚Ä¢ Total training time: {train_df['training_time'].sum():.2f} seconds ({train_df['training_time'].sum()/60:.1f} minutes)")
        print(f"   ‚Ä¢ Average training accuracy: {train_df['train_accuracy'].mean():.4f} ¬± {train_df['train_accuracy'].std():.4f}")
        print(f"   ‚Ä¢ Training time range: {train_df['training_time'].min():.2f} - {train_df['training_time'].max():.2f} seconds")
        
        print("\nüìà Testing Statistics:")
        print(f"   ‚Ä¢ Total test runs: {len(test_df)}")
        print(f"   ‚Ä¢ Average test accuracy: {test_df['accuracy'].mean():.4f} ¬± {test_df['accuracy'].std():.4f}")
        print(f"   ‚Ä¢ Average test precision: {test_df['precision'].mean():.4f} ¬± {test_df['precision'].std():.4f}")
        print(f"   ‚Ä¢ Average test recall: {test_df['recall'].mean():.4f} ¬± {test_df['recall'].std():.4f}")
        print(f"   ‚Ä¢ Average test F1-score: {test_df['f1_score'].mean():.4f} ¬± {test_df['f1_score'].std():.4f}")
        print(f"   ‚Ä¢ Average confidence: {test_df['avg_confidence'].mean():.4f} ¬± {test_df['avg_confidence'].std():.4f}")
        print(f"   ‚Ä¢ Accuracy range: {test_df['accuracy'].min():.4f} - {test_df['accuracy'].max():.4f}")
        
        # Per-iteration statistics
        iteration_stats = test_df.groupby('iteration').agg({
            'accuracy': ['mean', 'std'],
            'precision': ['mean', 'std'],
            'recall': ['mean', 'std'],
            'f1_score': ['mean', 'std']
        }).round(4)
        
        print(f"\nüìä Per-Iteration Consistency:")
        print(f"   ‚Ä¢ Most consistent iteration (lowest accuracy std): {iteration_stats[('accuracy', 'std')].idxmin()}")
        print(f"   ‚Ä¢ Best performing iteration (highest accuracy mean): {iteration_stats[('accuracy', 'mean')].idxmax()}")
        
        # Model stability assessment
        accuracy_std = test_df['accuracy'].std()
        if accuracy_std < 0.01:
            stability = "Excellent"
        elif accuracy_std < 0.02:
            stability = "Good"
        elif accuracy_std < 0.05:
            stability = "Fair"
        else:
            stability = "Variable"
        
        print(f"   ‚Ä¢ Overall model stability: {stability} (std: {accuracy_std:.4f})")
        
        return train_df, test_df, iteration_stats
    
    def create_fast_visualizations(self, train_df, test_df):
        """
        Create optimized visualizations for fast robust training results
        """
        print("\nüìä Creating fast training visualizations...")
        
        # Create figure with key visualizations
        fig = plt.figure(figsize=(20, 12))
        
        # 1. Test Accuracy Distribution
        plt.subplot(2, 4, 1)
        plt.hist(test_df['accuracy'], bins=15, alpha=0.7, color='lightcoral', edgecolor='black')
        plt.axvline(test_df['accuracy'].mean(), color='red', linestyle='--',
                   label=f'Mean: {test_df["accuracy"].mean():.4f}')
        plt.axvline(test_df['accuracy'].median(), color='orange', linestyle='--',
                   label=f'Median: {test_df["accuracy"].median():.4f}')
        plt.xlabel('Test Accuracy')
        plt.ylabel('Frequency')
        plt.title('Test Accuracy Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. Accuracy Over Iterations
        plt.subplot(2, 4, 2)
        iteration_means = test_df.groupby('iteration')['accuracy'].mean()
        iteration_stds = test_df.groupby('iteration')['accuracy'].std()
        plt.plot(iteration_means.index, iteration_means.values, 'b-o', alpha=0.7, linewidth=2, markersize=4)
        plt.fill_between(iteration_means.index, 
                        iteration_means.values - iteration_stds.values,
                        iteration_means.values + iteration_stds.values, 
                        alpha=0.3, color='blue')
        plt.xlabel('Iteration')
        plt.ylabel('Test Accuracy')
        plt.title('Test Accuracy Over Iterations')
        plt.grid(True, alpha=0.3)
        
        # 3. Performance Metrics Comparison
        plt.subplot(2, 4, 3)
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        means = [test_df[metric].mean() for metric in metrics]
        stds = [test_df[metric].std() for metric in metrics]
        
        x_pos = np.arange(len(metrics))
        bars = plt.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, 
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        
        for i, (mean, std) in enumerate(zip(means, stds)):
            plt.text(i, mean + std + 0.01, f'{mean:.3f}', 
                    ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(x_pos, [m.replace('_', ' ').title() for m in metrics])
        plt.ylabel('Score')
        plt.title('Performance Metrics')
        plt.ylim(0, 1.1)
        plt.grid(True, alpha=0.3, axis='y')
        
        # 4. Training Time Distribution
        plt.subplot(2, 4, 4)
        plt.hist(train_df['training_time'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(train_df['training_time'].mean(), color='red', linestyle='--', 
                   label=f'Mean: {train_df["training_time"].mean():.2f}s')
        plt.xlabel('Training Time (seconds)')
        plt.ylabel('Frequency')
        plt.title('Training Time Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. Confidence Distribution
        plt.subplot(2, 4, 5)
        plt.hist(test_df['avg_confidence'], bins=15, alpha=0.7, color='gold', edgecolor='black')
        plt.axvline(test_df['avg_confidence'].mean(), color='red', linestyle='--',
                   label=f'Mean: {test_df["avg_confidence"].mean():.3f}')
        plt.xlabel('Average Prediction Confidence')
        plt.ylabel('Frequency')
        plt.title('Prediction Confidence Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. Per-Class Accuracy Heatmap
        plt.subplot(2, 4, 6)
        per_class_data = np.array([result['per_class_accuracy'] for result in self.testing_results])
        per_class_mean = np.mean(per_class_data, axis=0)
        
        colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(self.class_names)))
        bars = plt.bar(range(len(self.class_names)), per_class_mean, color=colors, alpha=0.8)
        
        for i, (bar, acc) in enumerate(zip(bars, per_class_mean)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(range(len(self.class_names)), [str(name) for name in self.class_names], rotation=45)
        plt.ylabel('Average Accuracy')
        plt.title('Per-Class Accuracy')
        plt.grid(True, alpha=0.3, axis='y')
        
        # 7. Model Stability Analysis
        plt.subplot(2, 4, 7)
        iteration_stds = test_df.groupby('iteration')['accuracy'].std()
        plt.bar(iteration_stds.index, iteration_stds.values, alpha=0.7, color='mediumpurple')
        plt.axhline(iteration_stds.mean(), color='red', linestyle='--',
                   label=f'Mean Std: {iteration_stds.mean():.4f}')
        plt.xlabel('Iteration')
        plt.ylabel('Accuracy Standard Deviation')
        plt.title('Model Stability per Iteration')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 8. Summary Statistics
        plt.subplot(2, 4, 8)
        plt.axis('off')
        
        summary_stats = f"""
FAST ROBUST TRAINING SUMMARY

Training Iterations: {len(train_df)}
Test Runs per Iteration: 5
Total Test Runs: {len(test_df)}

PERFORMANCE:
‚Ä¢ Test Accuracy: {test_df['accuracy'].mean():.4f} ¬± {test_df['accuracy'].std():.4f}
‚Ä¢ Best Accuracy: {test_df['accuracy'].max():.4f}
‚Ä¢ Worst Accuracy: {test_df['accuracy'].min():.4f}

TIMING:
‚Ä¢ Total Training: {train_df['training_time'].sum()/60:.1f} min
‚Ä¢ Avg per Iteration: {train_df['training_time'].mean():.1f}s

STABILITY:
‚Ä¢ Accuracy Std: {test_df['accuracy'].std():.4f}
‚Ä¢ Confidence: {test_df['avg_confidence'].mean():.3f}

CLASSES: {len(self.class_names)}
        """
        
        plt.text(0.05, 0.95, summary_stats, transform=plt.gca().transAxes, 
                fontsize=11, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.suptitle('Fast Robust SVM Training Analysis - 20 Iterations √ó 5 Test Runs', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('fast_robust_svm_analysis_20_iterations.png', dpi=300, bbox_inches='tight')
        print("‚úÖ Fast training visualizations saved as 'fast_robust_svm_analysis_20_iterations.png'")
        plt.show()
        
        return fig
    
    def save_results(self, data_folder_path):
        """
        Save all results and the best model
        """
        print("\n" + "="*60)
        print("üíæ SAVING RESULTS AND BEST MODEL")
        print("="*60)
        
        try:
            # Save best model
            model_filename = os.path.join(data_folder_path, 'best_svm_model_fast_robust_20.pkl')
        
            model_data = {
                'model': self.best_model,
                'scaler': self.scaler,
                'label_encoder': self.label_encoder,
                'feature_names': self.feature_names,
                'class_names': self.class_names,
                'training_results': self.training_results,
                'testing_results': self.testing_results,
                'use_gpu': self.use_gpu,
                'base_random_state': self.base_random_state,
                'n_iterations': 20,
                'version': '2.0_fast_robust_20',
                'timestamp': datetime.now().isoformat()
            }
        
            with open(model_filename, 'wb') as f:
                pickle.dump(model_data, f)
        
            model_size = os.path.getsize(model_filename) / (1024*1024)
            print(f"‚úÖ Best model saved: {model_filename} ({model_size:.2f} MB)")
        
            # Save detailed results as JSON with proper type conversion
            results_filename = os.path.join(data_folder_path, 'fast_robust_training_results_20.json')
        
            def convert_numpy_types(obj):
                """Convert numpy types to native Python types for JSON serialization"""
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
        
            # Convert training results
            training_results_json = []
            for result in self.training_results:
                converted_result = convert_numpy_types(result)
                training_results_json.append(converted_result)
        
            # Calculate testing summary statistics
            test_accuracies = [float(r['accuracy']) for r in self.testing_results]
            test_confidences = [float(r['avg_confidence']) for r in self.testing_results]
        
            json_results = {
                'n_iterations': 20,
                'n_test_runs_per_iteration': 5,
                'total_test_runs': len(self.testing_results),
                'training_results': training_results_json,
                'testing_summary': {
                    'mean_accuracy': float(np.mean(test_accuracies)),
                    'std_accuracy': float(np.std(test_accuracies)),
                    'best_accuracy': float(max(test_accuracies)),
                    'worst_accuracy': float(min(test_accuracies)),
                    'mean_confidence': float(np.mean(test_confidences)),
                    'std_confidence': float(np.std(test_confidences))
                },
                'class_names': [str(name) for name in self.class_names],
                'feature_count': len(self.feature_names),
                'gpu_used': self.use_gpu,
                'timestamp': datetime.now().isoformat()
            }
        
            # Save detailed per-iteration results (optional, smaller version)
            iteration_results = []
            test_df = pd.DataFrame(self.testing_results)
            for iteration in range(1, 21):
                iter_data = test_df[test_df['iteration'] == iteration]
                if len(iter_data) > 0:
                    iteration_results.append({
                        'iteration': int(iteration),
                        'mean_accuracy': float(iter_data['accuracy'].mean()),
                        'std_accuracy': float(iter_data['accuracy'].std()),
                        'mean_confidence': float(iter_data['avg_confidence'].mean()),
                        'test_runs': len(iter_data)
                    })
        
            json_results['per_iteration_summary'] = iteration_results
        
            with open(results_filename, 'w') as f:
                json.dump(json_results, f, indent=2)
        
            results_size = os.path.getsize(results_filename) / (1024*1024)
            print(f"‚úÖ Results summary saved: {results_filename} ({results_size:.2f} MB)")
        
            # Also save a simple CSV summary for easy analysis
            csv_filename = os.path.join(data_folder_path, 'fast_robust_summary_20.csv')
            summary_data = []
        
            for result in self.testing_results:
                summary_data.append({
                    'iteration': int(result['iteration']),
                    'test_run': int(result['test_run']),
                    'accuracy': float(result['accuracy']),
                    'precision': float(result['precision']),
                    'recall': float(result['recall']),
                    'f1_score': float(result['f1_score']),
                    'avg_confidence': float(result['avg_confidence'])
                })
        
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(csv_filename, index=False)
            print(f"‚úÖ CSV summary saved: {csv_filename}")
        
            return model_filename, results_filename
        
        except Exception as e:
            print(f"‚ùå Error saving results: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def _convert_gpu_to_cpu(self, data):
        """Helper method to convert GPU data to CPU format"""
        if hasattr(data, 'to_pandas'):
            return data.to_pandas()
        elif hasattr(data, 'get'):  # cupy array
            return data.get()
        else:
            return data
    
    def print_final_summary(self):
        """Print comprehensive final summary"""
        print("\n" + "="*80)
        print("üéâ FAST ROBUST SVM TRAINING COMPLETED! (20 ITERATIONS)")
        print("="*80)
        
        if self.training_results and self.testing_results:
            train_df = pd.DataFrame(self.training_results)
            test_df = pd.DataFrame(self.testing_results)
            
            print(f"üìä Training Summary:")
            print(f"   ‚Ä¢ Total iterations: {len(train_df)}")
            print(f"   ‚Ä¢ Total test runs: {len(test_df)}")
            print(f"   ‚Ä¢ Average training time: {train_df['training_time'].mean():.2f} ¬± {train_df['training_time'].std():.2f} seconds")
            print(f"   ‚Ä¢ Total training time: {train_df['training_time'].sum():.2f} seconds ({train_df['training_time'].sum()/60:.1f} minutes)")
            
            print(f"\nüéØ Performance Summary:")
            print(f"   ‚Ä¢ Average test accuracy: {test_df['accuracy'].mean():.4f} ¬± {test_df['accuracy'].std():.4f}")
            print(f"   ‚Ä¢ Best test accuracy: {test_df['accuracy'].max():.4f}")
            print(f"   ‚Ä¢ Worst test accuracy: {test_df['accuracy'].min():.4f}")
            print(f"   ‚Ä¢ Accuracy stability (std): {test_df['accuracy'].std():.4f}")
            
            print(f"\nüìà Detailed Metrics:")
            print(f"   ‚Ä¢ Precision: {test_df['precision'].mean():.4f} ¬± {test_df['precision'].std():.4f}")
            print(f"   ‚Ä¢ Recall: {test_df['recall'].mean():.4f} ¬± {test_df['recall'].std():.4f}")
            print(f"   ‚Ä¢ F1-Score: {test_df['f1_score'].mean():.4f} ¬± {test_df['f1_score'].std():.4f}")
            print(f"   ‚Ä¢ Avg Confidence: {test_df['avg_confidence'].mean():.4f} ¬± {test_df['avg_confidence'].std():.4f}")
            
            print(f"\nüèÜ Best Model:")
            best_iter = test_df.groupby('iteration')['accuracy'].mean().idxmax()
            best_accuracy = test_df.groupby('iteration')['accuracy'].mean().max()
            print(f"   ‚Ä¢ Best iteration: {best_iter}")
            print(f"   ‚Ä¢ Best average accuracy: {best_accuracy:.4f}")
            
            print(f"\nüéØ Attack Types: {', '.join(map(str, self.class_names))}")
            
        print(f"\nüíæ Files Generated:")
        print(f"   ‚Ä¢ Best model: best_svm_model_fast_robust_20.pkl")
        print(f"   ‚Ä¢ Results summary: fast_robust_training_results_20.json")
        print(f"   ‚Ä¢ Visualizations: fast_robust_svm_analysis_20_iterations.png")
        
        print(f"\n‚úÖ Fast Model Robustness Verified:")
        print(f"   ‚Ä¢ Tested across 20 different data orderings")
        print(f"   ‚Ä¢ 5 test runs per training iteration")
        print(f"   ‚Ä¢ Total evaluations: 100 test runs")
        print(f"   ‚Ä¢ Testing performed AFTER all training completed")
        print(f"   ‚Ä¢ Optimized for speed while maintaining reliability")
        
        print("\nüöÄ Ready for production deployment!")
        print("="*80)


def main():
    """
    Main fast robust training pipeline with 20 iterations
    """
    print("üöÄ Fast Robust SVM Network Attack Detection System")
    print("üîÑ 20 Training Iterations √ó 5 Test Runs = 100 Total Evaluations")
    print("üß™ Testing runs AFTER all training is completed")
    print("‚ö° Optimized for speed (~1 hour total)")
    print("=" * 80)
    
    # Configuration
    DATA_FOLDER = '/content/drive/MyDrive/AI'
    TRAIN_PATH = os.path.join(DATA_FOLDER, 'train.csv')
    TEST_PATH = os.path.join(DATA_FOLDER, 'test.csv')
    
    N_ITERATIONS = 20   # Fast training with 20 iterations
    N_TEST_RUNS = 5     # Number of test runs per iteration
    
    # Initialize fast robust detector
    detector = FastRobustNetworkAttackDetector(use_gpu=True, base_random_state=42)
    
    try:
        # Step 1: Load and preprocess data
        train_df_processed, X_test_processed, y_test_encoded = detector.load_and_preprocess_data(TRAIN_PATH, TEST_PATH)
        
        # Step 2: Train all models FIRST
        detector.train_all_iterations(train_df_processed, n_iterations=N_ITERATIONS)
        
        # Step 3: Test all models AFTER training is completed
        detector.test_all_models(X_test_processed, y_test_encoded, n_test_runs=N_TEST_RUNS)
        
        # Step 4: Analyze results
        train_df, test_df, iteration_stats = detector.analyze_results()
        
        if train_df is not None and test_df is not None:
            # Step 5: Create fast visualizations
            detector.create_fast_visualizations(train_df, test_df)
            
            # Step 6: Save results and best model
            model_file, results_file = detector.save_results(DATA_FOLDER)
            
            # Step 7: Print final summary
            detector.print_final_summary()
        
        return detector
        
    except FileNotFoundError:
        print("‚ùå CSV files not found!")
        print("Please ensure your files are at:")
        print(f"   ‚Ä¢ {TRAIN_PATH}")
        print(f"   ‚Ä¢ {TEST_PATH}")
        return None
        
    except Exception as e:
        print(f"‚ùå Fast robust training failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # Run fast robust training with 20 iterations
    detector = main()
    
    if detector is not None:
        print("\nüéâ Fast robust training completed successfully!")
        print("‚úÖ Model stability verified across 20 iterations")
        print("‚úÖ Best model saved for deployment")
        print("üìä Fast analysis and visualizations generated")
        print("üß™ All testing performed AFTER training completion")
        print("‚ö° Optimized for speed while maintaining reliability")
    else:
        print("\n‚ùå Fast robust training failed. Please check the errors above.")




